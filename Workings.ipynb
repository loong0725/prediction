{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fa34a81",
   "metadata": {},
   "source": [
    "## importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1d562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3511f3b7",
   "metadata": {},
   "source": [
    "## importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ec4829",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:/project/Python/predication/train.csv'\n",
    "df = pd.read_csv(path)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c846492",
   "metadata": {},
   "source": [
    "## Data preprocessing & EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b619ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values(['store', 'item', 'date'])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dfb521",
   "metadata": {},
   "source": [
    "#### Store wise sales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db0d851",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group by quarterly periods and store, summing sales\n",
    "quarterly_sales = df.groupby([pd.Grouper(key='date', freq='Q'), 'store'])['sales'].sum().reset_index()\n",
    "\n",
    "# Plot with Seaborn\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.lineplot(data=quarterly_sales, x='date', y='sales', hue='store', palette='tab10', marker='o')\n",
    "\n",
    "# Set the plot\n",
    "plt.title('Quarterly Sales per Store (2013â€“2017)', fontsize=16)\n",
    "plt.xlabel('Quarter')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.legend(title='Store', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aad6672",
   "metadata": {},
   "source": [
    "#### Store-Item Wise sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9e4d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group by quarterly period, store, and item\n",
    "quarterly_sales_stores_item = df.groupby([pd.Grouper(key='date', freq='Q'), 'store', 'item'])['sales'].sum().reset_index()\n",
    "\n",
    "# Plot: One subplot per store, each line is an item\n",
    "g = sns.FacetGrid(quarterly_sales_stores_item, col=\"store\", col_wrap=3, height=4, aspect=1.5, sharey=False)\n",
    "g.map_dataframe(sns.lineplot, x=\"date\", y=\"sales\", hue=\"item\", palette=\"tab20\", marker='o')\n",
    "\n",
    "# Enhance appearance\n",
    "g.set_titles(\"Store {col_name}\")\n",
    "g.set_axis_labels(\"Quarter\", \"Sales\")\n",
    "g.add_legend(title=\"Item\")\n",
    "plt.subplots_adjust(top=0.92)\n",
    "g.fig.suptitle(\"Quarterly Sales by Item for Each Store (2013â€“2017)\", fontsize=16)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90e15da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding the data pattern item/store/year wise\n",
    "\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "# Group by year, store, item â€” then aggregate\n",
    "sales_summary = df.groupby(['year', 'store', 'item'])['sales'].agg(\n",
    "    avg_sales='mean',\n",
    "    max_sales='max',\n",
    "    min_sales='min'\n",
    ").reset_index()\n",
    "\n",
    "# Display the first few rows\n",
    "print(sales_summary.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0554845a",
   "metadata": {},
   "source": [
    "## Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c390765",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 30\n",
    "\n",
    "# Container for dfing samples\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "# Group by store and item\n",
    "for (store, item), group in df.groupby(['store', 'item']):\n",
    "    sales_series = group['sales'].values\n",
    "\n",
    "    for i in range(len(sales_series) - window_size):\n",
    "        X_list.append(sales_series[i:i+window_size])\n",
    "        y_list.append(sales_series[i+window_size])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(X_list, dtype=np.float32).reshape(-1, window_size, 1)\n",
    "y = np.array(y_list,dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "X_df, X_val, y_df, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0827a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "val_dataset = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0170707",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b4a690",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SalesLSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2):\n",
    "        super(SalesLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6a08e8",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3574a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_model(model, df_loader, val_loader, optimizer, criterion, epochs=10, device='cpu'):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.df()\n",
    "        df_loss = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_preds, val_targets = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                preds = model(X_batch)\n",
    "                val_preds.append(preds.detach().cpu().numpy())\n",
    "                val_targets.append(y_batch.detach().cpu().numpy())\n",
    "\n",
    "        val_preds = np.concatenate(val_preds, axis=0)\n",
    "        val_targets = np.concatenate(val_targets, axis=0)\n",
    "\n",
    "        val_mae = mean_absolute_error(val_targets, val_preds)\n",
    "        val_mse = np.mean((val_targets - val_preds) ** 2)\n",
    "        val_r2 = r2_score(val_targets, val_preds)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "              f\"Train Loss: {train_loss / len(train_loader):.4f} | \"\n",
    "              f\"Val MSE: {val_mse:.4f} | \"\n",
    "              f\"Val MAE: {val_mae:.4f} | \"\n",
    "              f\"Val RÂ²: {val_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc68e010",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674d3f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the trained model\n",
    "def load_model(model_class, model_path, device='cpu'):\n",
    "    model = model_class(input_size=1, hidden_size=64, num_layers=2)  # Initialize with the same parameters\n",
    "    model.load_state_dict(torch.load(model_path))  # Load saved weights\n",
    "    model.to(device)  # Move model to the desired device (GPU or CPU)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f608bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load  model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_path = 'sales_model.pth' \n",
    "model = load_model(SalesLSTM, model_path=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c05a2e",
   "metadata": {},
   "source": [
    "define precition function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3ecdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_n_days(model, input_seq, window_size=90, n_days=90, device='cpu'):\n",
    "    model.eval()\n",
    "    seq = input_seq.copy()\n",
    "    predictions = []\n",
    "    for _ in range(n_days):\n",
    "        X = torch.tensor(seq[-window_size:], dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(X).cpu().numpy().squeeze()\n",
    "        predictions.append(pred)\n",
    "        seq = np.append(seq, pred)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe70992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the test.csv\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "test_df['date'] = pd.to_datetime(test_df['date'])\n",
    "\n",
    "# according to the forecast days to set size\n",
    "window_size = 90\n",
    "\n",
    "# Construct each (store, item) input sequence\n",
    "input_sequences = {}\n",
    "for (store, item), group in df.groupby(['store', 'item']):\n",
    "    group = group.sort_values('date')\n",
    "    sales = group['sales'].values[-window_size:]\n",
    "    if len(sales) < window_size:\n",
    "        sales = np.pad(sales, (window_size - len(sales), 0), mode='constant')\n",
    "    input_sequences[(store, item)] = sales\n",
    "\n",
    "# predict the next 90days \n",
    "result_rows = []\n",
    "for (store, item), input_seq in input_sequences.items():\n",
    "    preds = predict_next_n_days(model, input_seq, window_size=90, n_days=90, device=device)\n",
    "    \n",
    "    # transfer the sales to integer\n",
    "    preds = np.array(preds).astype(int)\n",
    "\n",
    "    # get 90days sales\n",
    "    temp_df = test_df[(test_df['store'] == store) & (test_df['item'] == item)].copy()\n",
    "    \n",
    "    # give the value to sales\n",
    "    temp_df['sales'] = preds\n",
    "\n",
    "    result_rows.append(temp_df)\n",
    "\n",
    "# concat all the value\n",
    "final_df = pd.concat(result_rows, ignore_index=True)\n",
    "\n",
    "# check\n",
    "print(final_df[['store', 'item', 'date', 'sales']].tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb17de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the file \n",
    "final_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"âœ… Predictions saved to 'submission.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04dfb22",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad16fa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the file\n",
    "train = pd.read_csv('train.csv')\n",
    "submission = pd.read_csv('submission.csv')\n",
    "\n",
    "# preprocessing the date colum \n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "submission['date'] = pd.to_datetime(submission['date'])\n",
    "train['month'] = train['date'].dt.to_period('M')  # e.g., 2023-01\n",
    "submission['month'] = submission['date'].dt.to_period('M')\n",
    "\n",
    "# aggreate sales by store_id, item_id, month \n",
    "train_agg = train.groupby(['store', 'item', 'month'])['sales'].sum().reset_index()\n",
    "submission_agg = submission.groupby(['store', 'item', 'month'])['sales'].sum().reset_index()\n",
    "\n",
    "# merged data\n",
    "merged = pd.merge(train_agg, submission_agg, \n",
    "                  on=['store', 'item', 'month'], \n",
    "                  how='outer',  # make sure every value been saved\n",
    "                  suffixes=('_actual', '_predicted'))\n",
    "\n",
    "# rename the colum \n",
    "merged = merged.rename(columns={'sales_actual': 'actual_sales', 'sales_predicted': 'predicted_sales'})\n",
    "\n",
    "# transfer to str\n",
    "merged['month'] = merged['month'].astype(str)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cbc7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "print(merged[['actual_sales', 'predicted_sales']].isna().sum())\n",
    "print(merged.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7447589",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization : Comparing sales for a certain store_id and item_id by month\n",
    "def plot_sales_comparison(store, item,ax=None):\n",
    "    subset = merged[(merged['store'] == store) & (merged['item'] == item)]\n",
    "    if subset.empty:\n",
    "        print(f\"No data for store_id={store}, item_id={item}\")\n",
    "        return\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    ax.plot(subset['month'], subset['actual_sales'], label='Actual Sales', color='blue', marker='o')\n",
    "    ax.plot(subset['month'], subset['predicted_sales'], label='Predicted Sales', color='orange', linestyle='--', marker='x')\n",
    "    ax.set_xlabel('Month')\n",
    "    ax.set_ylabel('Sales')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "#  Get all store-item combinations\n",
    "store_item_combos = train_agg.groupby(['store', 'item']).size().reset_index().rename(columns={0: 'count'})\n",
    "num_combos_to_plot = 6  # number of sub_plot pre page \n",
    "num_combos = len(store_item_combos)\n",
    "\n",
    "# set page \n",
    "rows = 2\n",
    "cols = 3\n",
    "\n",
    "# count full page \n",
    "num_pages = np.ceil(num_combos / num_combos_to_plot)\n",
    "\n",
    "for page in range(int(num_pages)):\n",
    "    # make a sub_plot\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(12, 8))\n",
    "    axes = axes.flatten()  # å°†äºŒç»´æ•°ç»„å±•å¹³ï¼Œæ–¹ä¾¿è¿­ä»£\n",
    "\n",
    "    # calculate the combination range displayed on the current page\n",
    "    start_idx = page * num_combos_to_plot\n",
    "    end_idx = min((page + 1) * num_combos_to_plot, num_combos)\n",
    "    \n",
    "    for i, idx in enumerate(range(start_idx, end_idx)):\n",
    "        store = store_item_combos.loc[idx, 'store']\n",
    "        item = store_item_combos.loc[idx, 'item']\n",
    "        print(f\"\\nðŸ“Š Plotting for Store {store}, Item {item}\")\n",
    "\n",
    "        # choose \n",
    "        ax = axes[i]\n",
    "        plot_sales_comparison(store, item, ax)  # give ax\n",
    "\n",
    "        ax.set_title(f\"Store {store} - Item {item}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # show\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c951c0",
   "metadata": {},
   "source": [
    "#### MASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f436d2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mase(train, submission, cycle=30):\n",
    "\n",
    "    # data preprocess\n",
    "    def preprocess(df, name):\n",
    "        df = df.copy()\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df['sales'] = pd.to_numeric(df['sales'], errors='coerce')\n",
    "        df = df.dropna(subset=['sales'])\n",
    "        print(f\"[{name}] Training data: {len(df)} rows\")\n",
    "        return df\n",
    "\n",
    "    # data clean\n",
    "    train_clean = preprocess(train, \"train_data\")\n",
    "    pred_clean = preprocess(submission, \"predict_data\")\n",
    "    \n",
    "    # baseline forecast\n",
    "    base_pred = (\n",
    "        train_clean\n",
    "        .assign(\n",
    "            month_day=lambda x: x.date.dt.strftime('%m-%d')  \n",
    "        )\n",
    "        .groupby(['store', 'item', 'month_day'])\n",
    "        ['sales'].mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={'sales': 'base_forecast'})\n",
    "    )\n",
    "    \n",
    "    # merge train and predict\n",
    "    merged = pd.merge(\n",
    "        pred_clean.assign(\n",
    "            month_day=lambda x: x.date.dt.strftime('%m-%d')\n",
    "        ),\n",
    "        base_pred,\n",
    "        on=['store', 'item', 'month_day'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # calculate\n",
    "    results = []\n",
    "    for (store, item), group in merged.groupby(['store', 'item']):\n",
    "        # get train data\n",
    "        hist_data = train_clean[\n",
    "            (train_clean.store == store) & \n",
    "            (train_clean.item == item)\n",
    "        ].sort_values('date')['sales'].values\n",
    "        \n",
    "        # set cycle\n",
    "        if len(hist_data) < 2 * cycle:\n",
    "            continue\n",
    "            \n",
    "        # split \n",
    "        base_period = hist_data[:-cycle]\n",
    "        actual_period = hist_data[-cycle:]\n",
    "        \n",
    "        # Calculate the baseline forecast\n",
    "        base_forecasts = []\n",
    "        for i in range(cycle):\n",
    "            seasonal_values = base_period[i::cycle]\n",
    "            if len(seasonal_values) > 0:\n",
    "                base_forecasts.append(np.mean(seasonal_values))\n",
    "            else:\n",
    "                base_forecasts.append(np.nan)\n",
    "        \n",
    "        # benchmark error\n",
    "        valid_mask = ~np.isnan(base_forecasts)\n",
    "        naive_errors = np.nanmean(\n",
    "            np.abs(actual_period[valid_mask] - np.array(base_forecasts)[valid_mask])\n",
    "        )\n",
    "        \n",
    "        # Prediction Error\n",
    "        pred_errors = np.abs(group['sales'] - group['base_forecast']).mean()\n",
    "        \n",
    "        # MASE\n",
    "        if naive_errors > 0:\n",
    "            mase = pred_errors / naive_errors\n",
    "        else:\n",
    "            mase = np.nan\n",
    "        \n",
    "        results.append({\n",
    "            'store': store,\n",
    "            'item': item,\n",
    "            'mase': round(mase, 2),\n",
    "            'pred_sales_mean': group['sales'].mean(),\n",
    "            'base_sales_mean': group['base_forecast'].mean(),\n",
    "            'valid_days': len(group)\n",
    "        })\n",
    "    \n",
    "    # analize the result \n",
    "    results_df = pd.DataFrame(results)\n",
    "    global_stats = {\n",
    "        'total_combinations': train_clean.groupby(['store', 'item']).ngroups,\n",
    "        'valid_combinations': len(results_df),\n",
    "        'mean_mase': results_df.mase.mean(),\n",
    "        'median_mase': results_df.mase.median(),\n",
    "        'coverage_ratio': f\"{len(results_df)/train_clean.groupby(['store', 'item']).ngroups:.1%}\"\n",
    "    }\n",
    "    \n",
    "    return results_df, global_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc1f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the mase funtion\n",
    "if __name__ == \"__main__\":\n",
    "    results, stats = calculate_mase(train, submission)\n",
    "    \n",
    "    print(\"\\nGlobal statistics:\")\n",
    "    print(f\"- Total number of combinations: {stats['total_combinations']}\")\n",
    "    print(f\"- Effective combination coverage: {stats['coverage_ratio']}\")\n",
    "    print(f\"- Average MASE: {stats['mean_mase']:.2f}\")\n",
    "    print(f\"- Median MASE: {stats['median_mase']:.2f}\")\n",
    "\n",
    "    print(\"\\nDetailed results:\")\n",
    "    print(results.sort_values('mase').head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
